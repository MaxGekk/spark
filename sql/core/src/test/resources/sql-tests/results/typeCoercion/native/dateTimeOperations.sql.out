-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TEMPORARY VIEW t AS SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
select cast(1 as tinyint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as smallint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as int) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as bigint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as float) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as double) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as decimal(10, 0)) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast('2017-12-11' as string) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 00:00:00


-- !query
select cast('2017-12-11 09:30:00' as string) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 09:30:00


-- !query
select cast('1' as binary) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as boolean) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast('2017-12-11 09:30:00.0' as timestamp) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) + INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-13 09:30:00


-- !query
select cast('2017-12-11 09:30:00' as date) + interval 2 day
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), extractansiintervaldays(INTERVAL '2' DAY)):date>
-- !query output
2017-12-13


-- !query
select interval 2 day + cast(1 as tinyint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as smallint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as int)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as bigint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as float)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as double)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as decimal(10, 0))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast('2017-12-11' as string)
-- !query schema
struct<CAST(2017-12-11 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 00:00:00


-- !query
select interval 2 day + cast('2017-12-11 09:30:00' as string)
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 09:30:00


-- !query
select interval 2 day + cast('1' as binary)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast(1 as boolean)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select interval 2 day + cast('2017-12-11 09:30:00.0' as timestamp)
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) + INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-13 09:30:00


-- !query
select interval 2 day + cast('2017-12-11 09:30:00' as date)
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), extractansiintervaldays(INTERVAL '2' DAY)):date>
-- !query output
2017-12-13


-- !query
select cast(1 as tinyint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as smallint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as int) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as bigint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as float) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as double) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as decimal(10, 0)) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast('2017-12-11' as string) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 AS STRING) - INTERVAL '2' DAY:string>
-- !query output
2017-12-09 00:00:00


-- !query
select cast('2017-12-11 09:30:00' as string) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) - INTERVAL '2' DAY:string>
-- !query output
2017-12-09 09:30:00


-- !query
select cast('1' as binary) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast(1 as boolean) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{"errorClass":null,"messageParameters":[],"queryContext":[]}


-- !query
select cast('2017-12-11 09:30:00.0' as timestamp) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) - INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-09 09:30:00


-- !query
select cast('2017-12-11 09:30:00' as date) - interval 2 day
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), (- extractansiintervaldays(INTERVAL '2' DAY))):date>
-- !query output
2017-12-09
