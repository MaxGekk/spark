-- Automatically generated by SQLQueryTestSuite
-- !query
SELECT try_add(1, 1)
-- !query schema
struct<try_add(1, 1):int>
-- !query output
2


-- !query
SELECT try_add(2147483647, 1)
-- !query schema
struct<try_add(2147483647, 1):int>
-- !query output
NULL


-- !query
SELECT try_add(-2147483648, -1)
-- !query schema
struct<try_add(- 2147483648, - 1):int>
-- !query output
NULL


-- !query
SELECT try_add(9223372036854775807L, 1)
-- !query schema
struct<try_add(9223372036854775807L, 1):bigint>
-- !query output
NULL


-- !query
SELECT try_add(-9223372036854775808L, -1)
-- !query schema
struct<try_add(- 9223372036854775808L, - 1):bigint>
-- !query output
NULL


-- !query
SELECT try_add(1, (2147483647 + 1))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "integer overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 20,
    "stopIndex" : 33,
    "fragment" : "2147483647 + 1"
  } ]
}


-- !query
SELECT try_add(1L, (9223372036854775807L + 1L))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "long overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 21,
    "stopIndex" : 45,
    "fragment" : "9223372036854775807L + 1L"
  } ]
}


-- !query
SELECT try_add(1, 1.0 / 0.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 27,
    "fragment" : "1.0 / 0.0"
  } ]
}


-- !query
SELECT try_add(date'2021-01-01', 1)
-- !query schema
struct<try_add(date '2021-01-01', 1):date>
-- !query output
2021-01-02


-- !query
SELECT try_add(1, date'2021-01-01')
-- !query schema
struct<try_add(1, date '2021-01-01'):date>
-- !query output
2021-01-02


-- !query
SELECT try_add(date'2021-01-01', interval 2 year)
-- !query schema
struct<try_add(date '2021-01-01', interval 2 year):date>
-- !query output
2023-01-01


-- !query
SELECT try_add(date'2021-01-01', interval 2 second)
-- !query schema
struct<try_add(date '2021-01-01', interval 2 second):timestamp>
-- !query output
2021-01-01 00:00:02


-- !query
SELECT try_add(interval 2 year, date'2021-01-01')
-- !query schema
struct<try_add(interval 2 year, date '2021-01-01'):date>
-- !query output
2023-01-01


-- !query
SELECT try_add(interval 2 second, date'2021-01-01')
-- !query schema
struct<try_add(interval 2 second, date '2021-01-01'):timestamp>
-- !query output
2021-01-01 00:00:02


-- !query
SELECT try_add(timestamp_ltz'2021-01-01 00:00:00', interval 2 year)
-- !query schema
struct<try_add(timestamp_ltz '2021-01-01 00:00:00', interval 2 year):timestamp>
-- !query output
2023-01-01 00:00:00


-- !query
SELECT try_add(timestamp_ntz'2021-01-01 00:00:00', interval 2 second)
-- !query schema
struct<try_add(timestamp_ntz '2021-01-01 00:00:00', interval 2 second):timestamp_ntz>
-- !query output
2021-01-01 00:00:02


-- !query
SELECT try_add(interval 2 year, timestamp_ltz'2021-01-01 00:00:00')
-- !query schema
struct<try_add(interval 2 year, timestamp_ltz '2021-01-01 00:00:00'):timestamp>
-- !query output
2023-01-01 00:00:00


-- !query
SELECT try_add(interval 2 second, timestamp_ntz'2021-01-01 00:00:00')
-- !query schema
struct<try_add(interval 2 second, timestamp_ntz '2021-01-01 00:00:00'):timestamp_ntz>
-- !query output
2021-01-01 00:00:02


-- !query
SELECT try_add(interval 2 year, interval 2 year)
-- !query schema
struct<try_add(interval 2 year, interval 2 year):interval year>
-- !query output
4-0


-- !query
SELECT try_add(interval 2 second, interval 2 second)
-- !query schema
struct<try_add(interval 2 second, interval 2 second):interval second>
-- !query output
0 00:00:04.000000000


-- !query
SELECT try_add(interval 2 year, interval 2 second)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "messageParameters" : {
    "inputSql" : "\"INTERVAL '2' YEAR\"",
    "inputType" : "\"INTERVAL YEAR\"",
    "paramIndex" : "1",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"INTERVAL '2' YEAR + INTERVAL '02' SECOND\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 50,
    "fragment" : "try_add(interval 2 year, interval 2 second)"
  } ]
}


-- !query
SELECT try_add(interval 2147483647 month, interval 2 month)
-- !query schema
struct<try_add(interval 2147483647 month, interval 2 month):interval month>
-- !query output
NULL


-- !query
SELECT try_add(interval 106751991 day, interval 3 day)
-- !query schema
struct<try_add(interval 106751991 day, interval 3 day):interval day>
-- !query output
NULL


-- !query
SELECT try_divide(1, 0.5)
-- !query schema
struct<try_divide(1, 0.5):decimal(8,6)>
-- !query output
2.000000


-- !query
SELECT try_divide(1, 0)
-- !query schema
struct<try_divide(1, 0):double>
-- !query output
NULL


-- !query
SELECT try_divide(0, 0)
-- !query schema
struct<try_divide(0, 0):double>
-- !query output
NULL


-- !query
SELECT try_divide(1, (2147483647 + 1))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "integer overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 23,
    "stopIndex" : 36,
    "fragment" : "2147483647 + 1"
  } ]
}


-- !query
SELECT try_divide(1L, (9223372036854775807L + 1L))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "long overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 48,
    "fragment" : "9223372036854775807L + 1L"
  } ]
}


-- !query
SELECT try_divide(1, 1.0 / 0.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 22,
    "stopIndex" : 30,
    "fragment" : "1.0 / 0.0"
  } ]
}


-- !query
SELECT try_divide(interval 2 year, 2)
-- !query schema
struct<try_divide(interval 2 year, 2):interval year to month>
-- !query output
1-0


-- !query
SELECT try_divide(interval 2 second, 2)
-- !query schema
struct<try_divide(interval 2 second, 2):interval day to second>
-- !query output
0 00:00:01.000000000


-- !query
SELECT try_divide(interval 2 year, 0)
-- !query schema
struct<try_divide(interval 2 year, 0):interval year to month>
-- !query output
NULL


-- !query
SELECT try_divide(interval 2 second, 0)
-- !query schema
struct<try_divide(interval 2 second, 0):interval day to second>
-- !query output
NULL


-- !query
SELECT try_divide(interval 2147483647 month, 0.5)
-- !query schema
struct<try_divide(interval 2147483647 month, 0.5):interval year to month>
-- !query output
NULL


-- !query
SELECT try_divide(interval 106751991 day, 0.5)
-- !query schema
struct<try_divide(interval 106751991 day, 0.5):interval day to second>
-- !query output
NULL


-- !query
SELECT try_subtract(1, 1)
-- !query schema
struct<try_subtract(1, 1):int>
-- !query output
0


-- !query
SELECT try_subtract(2147483647, -1)
-- !query schema
struct<try_subtract(2147483647, - 1):int>
-- !query output
NULL


-- !query
SELECT try_subtract(-2147483648, 1)
-- !query schema
struct<try_subtract(- 2147483648, 1):int>
-- !query output
NULL


-- !query
SELECT try_subtract(9223372036854775807L, -1)
-- !query schema
struct<try_subtract(9223372036854775807L, - 1):bigint>
-- !query output
NULL


-- !query
SELECT try_subtract(-9223372036854775808L, 1)
-- !query schema
struct<try_subtract(- 9223372036854775808L, 1):bigint>
-- !query output
NULL


-- !query
SELECT try_subtract(1, (2147483647 + 1))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "integer overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 38,
    "fragment" : "2147483647 + 1"
  } ]
}


-- !query
SELECT try_subtract(1L, (9223372036854775807L + 1L))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "long overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 50,
    "fragment" : "9223372036854775807L + 1L"
  } ]
}


-- !query
SELECT try_subtract(1, 1.0 / 0.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 32,
    "fragment" : "1.0 / 0.0"
  } ]
}


-- !query
SELECT try_subtract(interval 2 year, interval 3 year)
-- !query schema
struct<try_subtract(interval 2 year, interval 3 year):interval year>
-- !query output
-1-0


-- !query
SELECT try_subtract(interval 3 second, interval 2 second)
-- !query schema
struct<try_subtract(interval 3 second, interval 2 second):interval second>
-- !query output
0 00:00:01.000000000


-- !query
SELECT try_subtract(interval 2147483647 month, interval -2 month)
-- !query schema
struct<try_subtract(interval 2147483647 month, interval - 2 month):interval month>
-- !query output
NULL


-- !query
SELECT try_subtract(interval 106751991 day, interval -3 day)
-- !query schema
struct<try_subtract(interval 106751991 day, interval - 3 day):interval day>
-- !query output
NULL


-- !query
SELECT try_multiply(2, 3)
-- !query schema
struct<try_multiply(2, 3):int>
-- !query output
6


-- !query
SELECT try_multiply(2147483647, -2)
-- !query schema
struct<try_multiply(2147483647, - 2):int>
-- !query output
NULL


-- !query
SELECT try_multiply(-2147483648, 2)
-- !query schema
struct<try_multiply(- 2147483648, 2):int>
-- !query output
NULL


-- !query
SELECT try_multiply(9223372036854775807L, 2)
-- !query schema
struct<try_multiply(9223372036854775807L, 2):bigint>
-- !query output
NULL


-- !query
SELECT try_multiply(-9223372036854775808L, -2)
-- !query schema
struct<try_multiply(- 9223372036854775808L, - 2):bigint>
-- !query output
NULL


-- !query
SELECT try_multiply(1, (2147483647 + 1))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "integer overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 38,
    "fragment" : "2147483647 + 1"
  } ]
}


-- !query
SELECT try_multiply(1L, (9223372036854775807L + 1L))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "alternative" : " Use 'try_add' to tolerate overflow and return NULL instead.",
    "config" : "\"spark.sql.ansi.enabled\"",
    "message" : "long overflow"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 50,
    "fragment" : "9223372036854775807L + 1L"
  } ]
}


-- !query
SELECT try_multiply(1, 1.0 / 0.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 32,
    "fragment" : "1.0 / 0.0"
  } ]
}


-- !query
SELECT try_multiply(interval 2 year, 2)
-- !query schema
struct<try_multiply(interval 2 year, 2):interval year to month>
-- !query output
4-0


-- !query
SELECT try_multiply(interval 2 second, 2)
-- !query schema
struct<try_multiply(interval 2 second, 2):interval day to second>
-- !query output
0 00:00:04.000000000


-- !query
SELECT try_multiply(interval 2 year, 0)
-- !query schema
struct<try_multiply(interval 2 year, 0):interval year to month>
-- !query output
0-0


-- !query
SELECT try_multiply(interval 2 second, 0)
-- !query schema
struct<try_multiply(interval 2 second, 0):interval day to second>
-- !query output
0 00:00:00.000000000


-- !query
SELECT try_multiply(interval 2147483647 month, 2)
-- !query schema
struct<try_multiply(interval 2147483647 month, 2):interval year to month>
-- !query output
NULL


-- !query
SELECT try_multiply(interval 106751991 day, 2)
-- !query schema
struct<try_multiply(interval 106751991 day, 2):interval day to second>
-- !query output
NULL
